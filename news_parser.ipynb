{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список ключевых слов\n",
    "keywords = [\n",
    "    \"Коррупция\", \"Взятка\", \"Подкуп\", \"Злоупотребление полномочиями\", \"Непотизм\", \"Бездействие\",\n",
    "    \"Конфликт интересов\", \"Теневая экономика\", \"Финансовые махинации\", \"Противодействие коррупции\", \n",
    "    \"Транспарентность\", \"Подотчетность\", \"Мониторинг\", \"Антикоррупционная политика\", \"приговор\", \"дело\",\n",
    "    \"Прозрачность финансирования\", \"Теневые сделки\", \"Откаты\", \"Хищение\", \"Родственник\", \"расследуется\", \"расследование\",\n",
    "    \"Лоббизм\", \"Незаконное\", \"обогащение\", \"Контроль расходов\", \"Проверки деятельности\", \"подозреваемый\", \"подоздревается\",\n",
    "    \"Аудит\", \"Закупка\", \"Электронное правительство\", \"Похищение\", \"Отмывание\", \"Задержание\", \"Рейдерство\", \"тенге\", \"долларов\", \"доллары\",\n",
    "    \"Превышение власти\", \"Задержали\", \"Антикор\", \"Агенство\", \"Уголовный кодекс\", \"Получение\", \"незаконного\", \"вознаграждения\"\n",
    "    \"Концепция\", \"Преступление\", \"средней тяжести\", \"небольшой тяжести\", \"Тяжкие\", \"Особо тяжкие\", \"Мошенничество\", \"Легализация\",\n",
    "    \"миллиард\", \"миллион\", \"триллиард\", \"Обогатился\", \"Обогатились\", \"судебный\", \"процесс\", \"аким\", \"начальник\", \"зам\", \"заместитель\",\n",
    "    \"район\", \"город\", \"область\", \"село\", \"деревня\", \"Аул\", \"правонарушение\", \"Антикора\", \"Антикору\", \"Антикором\", \"Антикоре\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(columns=[\"Date\"] + keywords)\n",
    "\n",
    "# Инициализация pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Генерация всех форм ключевых слов\n",
    "def generate_forms(word):\n",
    "    parsed = morph.parse(word)[0]\n",
    "    return {form.word for form in parsed.lexeme}\n",
    "\n",
    "# Генерируем формы для ключевых слов\n",
    "keyword_forms = {}\n",
    "for keyword in keywords:\n",
    "    keyword_forms[keyword.lower()] = generate_forms(keyword.lower())\n",
    "\n",
    "# Объединяем все формы в один набор\n",
    "all_keyword_forms = set()\n",
    "for forms in keyword_forms.values():\n",
    "    all_keyword_forms.update(forms)\n",
    "\n",
    "\n",
    "# Функция подсчёта общего количества вхождений ключевых слов\n",
    "def find_total_keyword_occurrences(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())  # Извлекаем только слова\n",
    "    lemmatized_tokens = [morph.parse(token)[0].normal_form for token in tokens]  # Лемматизация\n",
    "    \n",
    "    # Подсчитываем общее количество вхождений ключевых слов\n",
    "    word_count = defaultdict(int)\n",
    "    for word in lemmatized_tokens:\n",
    "        if word in all_keyword_forms:\n",
    "            word_count[word] += 1  # Увеличиваем счётчик для каждого вхождения\n",
    "    \n",
    "    return word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url1 = \"https://tengrinews.kz\"\n",
    "\n",
    "# Загрузка ссылок из CSV\n",
    "links_df = pd.read_csv(\"news_links.csv\")\n",
    "total_links = len(links_df)\n",
    "chunk_size = total_links // 5  # Делим на 5 частей\n",
    "\n",
    "# Укажите начальный индекс для итерации\n",
    "start_index = chunk_size * 2  # Изменяйте при каждом запуске\n",
    "\n",
    "# Парсинг ссылок\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in links_df.iloc[start_index:start_index + chunk_size].iterrows():\n",
    "    url = row[\"News Link\"]\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = f\"{base_url1}{url}\"  # Добавляем домен к относительным ссылкам\n",
    "\n",
    "\n",
    "    print(f\"Парсим новость: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Ошибка при загрузке {url}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Извлекаем дату публикации\n",
    "        date_element = soup.find('div', {'class': 'date-time'})\n",
    "        date = date_element.get_text(strip=True).split('|')[0].strip() if date_element else \"Unknown\"\n",
    "\n",
    "        # Извлекаем текст новости\n",
    "        article_body = soup.find('div', {'class': 'content_main_text'})\n",
    "        text = article_body.get_text(strip=True) if article_body else \"\"\n",
    "\n",
    "        # Подсчитываем ключевые слова\n",
    "        keyword_counts = find_total_keyword_occurrences(text)\n",
    "\n",
    "        # Формируем результаты\n",
    "        result = {\"Дата\": date, \"URL\": url}\n",
    "        for keyword in keywords:\n",
    "            result[keyword] = keyword_counts.get(keyword.lower(), 0)\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "        # Пауза для избежания блокировок\n",
    "        time.sleep(random.uniform(2, 6))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке {url}: {e}\")\n",
    "\n",
    "# Сохраняем результаты в CSV\n",
    "iteration_number = start_index // chunk_size + 1\n",
    "output_file = f\"news_results_part{iteration_number}.csv\"\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"Результаты сохранены в '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
