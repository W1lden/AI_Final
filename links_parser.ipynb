{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tengrinews.kz/tag/%D0%BA%D0%BE%D1%80%D1%80%D1%83%D0%BF%D1%86%D0%B8%D1%8F/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://tengrinews.kz/tag/коррупция/\"\n",
    "total_pages = 102\n",
    "all_news_links = []\n",
    "\n",
    "# Переход по страницам\n",
    "for page in range(1, total_pages + 1):\n",
    "    # Формируем URL страницы\n",
    "    if page == 1:\n",
    "        url = base_url  # Первая страница без /page/\n",
    "    else:\n",
    "        url = f\"{base_url}page/{page}/\"\n",
    "    \n",
    "    print(f\"Парсим страницу: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Ошибка при загрузке страницы {url}\")\n",
    "            continue\n",
    "\n",
    "        # Парсим HTML с BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Извлекаем ссылки на новости\n",
    "        news_list = soup.find_all('div', {'class': 'content_main_item'})\n",
    "        for news_item in news_list:\n",
    "            first_link = news_item.find('a', href=True)\n",
    "            if first_link:\n",
    "                all_news_links.append(first_link['href'])\n",
    "        \n",
    "\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке страницы {url}: {e}\")\n",
    "\n",
    "# Убираем дублирующиеся ссылки\n",
    "all_news_links = list(set(all_news_links))\n",
    "print(f\"Найдено {len(all_news_links)} ссылок на новости.\")\n",
    "\n",
    "# Сохраняем ссылки в CSV-файл\n",
    "links_df = pd.DataFrame(all_news_links, columns=[\"News Link\"])\n",
    "links_df.to_csv(\"news_links.csv\", index=False, encoding='utf-8')\n",
    "print(\"Ссылки сохранены в файл 'news_links.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
